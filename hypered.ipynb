{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOomS9VWBGPlQKoElLG+7iF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ebrahim-Ghaith/Research/blob/main/hypered.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class Net_m_task_CNN(nn.Module):\n",
        "    def __init__(self, n_in, n_hidden, n_out_Reg, n_out_clas, p_dropout, U, Ass, out_channel, kernel_s, padding):\n",
        "        super(Net_m_task_CNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_channel, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=out_channel)\n",
        "        self.do1 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu2 = nn.LeakyReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=out_channel)\n",
        "        self.do2 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "        self.cnn3 = nn.Conv2d(in_channels=out_channel, out_channels=8, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=8)\n",
        "        self.do3 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "        # Fully connected 1 (readout)\n",
        "        x_new = (U + 2 * padding - kernel_s) + 1\n",
        "        y_new = (Ass + 2 * padding - kernel_s) + 1\n",
        "\n",
        "        nn_in_fc = 8 * (x_new + 2 * padding - kernel_s + 1) * (y_new + 2 * padding - kernel_s + 1)\n",
        "\n",
        "        self.fc20 = nn.Linear(nn_in_fc, n_hidden)\n",
        "        self.bn20 = nn.BatchNorm1d(n_hidden)\n",
        "        self.relu20 = nn.LeakyReLU()\n",
        "        self.do20 = nn.Dropout(p_dropout)\n",
        "\n",
        "        self.fc30 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.bn30 = nn.BatchNorm1d(n_hidden)\n",
        "        self.relu30 = nn.LeakyReLU()\n",
        "        self.do30 = nn.Dropout(p_dropout)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc7R = nn.Linear(n_hidden, n_out_Reg)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc8R = nn.Linear(n_hidden, n_out_Reg)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc9C = nn.Linear(n_hidden, n_out_clas)\n",
        "\n",
        "    def forward(self, x):  # always\n",
        "\n",
        "        out = self.cnn1(x)\n",
        "        out = self.do1(out)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.cnn2(out)\n",
        "        out = self.do2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        out = self.cnn3(out)\n",
        "        out = self.do3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu3(out)\n",
        "\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        out = self.fc20(out)\n",
        "        out = self.do20(out)\n",
        "        out = self.relu20(out)\n",
        "        out = self.bn20(out)\n",
        "\n",
        "        out = self.fc30(out)\n",
        "        out = self.do30(out)\n",
        "        out = self.relu30(out)\n",
        "        out = self.bn30(out)\n",
        "\n",
        "        # Linear function (readout)  ****** LINEAR ******\n",
        "        outR1 = self.fc7R(out)\n",
        "\n",
        "        # Linear function (readout)  ****** LINEAR ******\n",
        "        outR2 = self.fc8R(out)\n",
        "\n",
        "        # Linear function (readout)  ****** LINEAR ******\n",
        "        outC = self.fc9C(out)\n",
        "\n",
        "        return outR1, outR2, outC\n"
      ],
      "metadata": {
        "id": "hTtS9bD3-EPN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from networks_arch import Net_m_task_CNN\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class Networks_activations(object):\n",
        "    def __init__(self,\n",
        "                 Us,\n",
        "                 Mr,\n",
        "                 Nrf,\n",
        "                 K,\n",
        "                 K_limited,\n",
        "                 Noise_pwr,\n",
        "                 device,\n",
        "                 device_ids,\n",
        "                 n_input,\n",
        "                 n_hidden,\n",
        "                 n_output_reg,\n",
        "                 n_output_clas,\n",
        "                 p_dropout,\n",
        "                 out_channel,\n",
        "                 kernel_s,\n",
        "                 padding\n",
        "                 ):\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.K = K\n",
        "        self.K_limited = K_limited\n",
        "        self.Noise_pwr = Noise_pwr\n",
        "        self.device = device\n",
        "        self.dev_id = device_ids\n",
        "        self.n_input = n_input\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output_reg = n_output_reg\n",
        "        self.n_output_clas = n_output_clas\n",
        "        self.p_dropout = p_dropout\n",
        "        self.out_channel = out_channel\n",
        "        self.kernel_s = kernel_s\n",
        "        self.padding = padding\n",
        "\n",
        "    def Network_m_Task(self):\n",
        "        if self.device.type == 'cuda':\n",
        "            return nn.DataParallel(Net_m_task_CNN(self.n_input, self.n_hidden, self.n_output_reg, self.n_output_clas, self.p_dropout, self.Us, self.K_limited, self.out_channel, self.kernel_s, self.padding), device_ids=self.dev_id).to(self.device)\n",
        "        else:\n",
        "            return Net_m_task_CNN(self.n_input, self.n_hidden, self.n_output_reg, self.n_output_clas,\n",
        "                self.p_dropout, self.Us, self.K_limited, self.out_channel, self.kernel_s, self.padding)\n",
        "\n",
        "    def Inp_MT(self, RSSI):\n",
        "        Inputs_MT = RSSI.reshape(len(RSSI), 1, self.Us, self.K)[:, :, :, 0:self.K_limited].float().to(self.device)\n",
        "        return Inputs_MT\n"
      ],
      "metadata": {
        "id": "vKPRRcKl-BeV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@author: Hamed Hojatian\n",
        "\n",
        "Complex aljebra for pytorch in case that pytorch\n",
        "version doesn't support complex tensor (V 1.8 and\n",
        "later support complex tensor and aljebra)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from numpy import genfromtxt\n",
        "import numpy as np\n",
        "\n",
        "def Th_comp_matmul(Ar, Ai, Br, Bi):  # Complex matmul pytorch function ########\n",
        "    if Ar.ndim == 3 and Br.ndim == 3:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=2), torch.cat((Ai, Ar), dim=2)), dim=1)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=2), torch.cat((Bi, Br), dim=2)), dim=1)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[:, 0:int(c_th.shape[1] / 2), 0:int(c_th.shape[2] / 2)]\n",
        "        c_th_i = c_th[:, int(c_th.shape[1] / 2):, 0:int(c_th.shape[2] / 2)]\n",
        "    elif Ar.ndim == 2 and Br.ndim == 2:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=1), torch.cat((Ai, Ar), dim=1)), dim=0)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=1), torch.cat((Bi, Br), dim=1)), dim=0)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[0:int(c_th.shape[0] / 2), 0:int(c_th.shape[1] / 2)]\n",
        "        c_th_i = c_th[int(c_th.shape[0] / 2):, 0:int(c_th.shape[1] / 2)]\n",
        "    elif Ar.ndim == 4 and Br.ndim == 4:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=3), torch.cat((Ai, Ar), dim=3)), dim=2)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=3), torch.cat((Bi, Br), dim=3)), dim=2)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[:, :, 0:int(c_th.shape[2] / 2), 0:int(c_th.shape[3] / 2)]\n",
        "        c_th_i = c_th[:, :, int(c_th.shape[2] / 2):, 0:int(c_th.shape[3] / 2)]\n",
        "    elif Ar.ndim == 5 and Br.ndim == 5:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=4), torch.cat((Ai, Ar), dim=4)), dim=3)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=4), torch.cat((Bi, Br), dim=4)), dim=3)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[:, :, :, 0:int(c_th.shape[3] / 2), 0:int(c_th.shape[4] / 2)]\n",
        "        c_th_i = c_th[:, :, :, int(c_th.shape[3] / 2):, 0:int(c_th.shape[4] / 2)]\n",
        "    elif Ar.ndim * Br.ndim == 12:\n",
        "        if Ar.ndim == 4:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=3), torch.cat((Ai, Ar), dim=3)), dim=2)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=2), torch.cat((Bi, Br), dim=2)), dim=1)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, 0:int(c_th.shape[2] / 2), 0:int(c_th.shape[3] / 2)]\n",
        "            c_th_i = c_th[:, :, int(c_th.shape[2] / 2):, 0:int(c_th.shape[3] / 2)]\n",
        "        elif Br.ndim == 4:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=2), torch.cat((Ai, Ar), dim=2)), dim=1)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=3), torch.cat((Bi, Br), dim=3)), dim=2)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, 0:int(c_th.shape[2] / 2), 0:int(c_th.shape[3] / 2)]\n",
        "            c_th_i = c_th[:, :, int(c_th.shape[2] / 2):, 0:int(c_th.shape[3] / 2)]\n",
        "    elif Ar.ndim * Br.ndim == 20:\n",
        "        if Ar.ndim == 5:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=4), torch.cat((Ai, Ar), dim=4)), dim=3)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=3), torch.cat((Bi, Br), dim=3)), dim=2)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, :, 0:int(c_th.shape[3] / 2), 0:int(c_th.shape[4] / 2)]\n",
        "            c_th_i = c_th[:, :, :, int(c_th.shape[3] / 2):, 0:int(c_th.shape[4] / 2)]\n",
        "        elif Br.ndim == 5:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=3), torch.cat((Ai, Ar), dim=3)), dim=2)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=4), torch.cat((Bi, Br), dim=4)), dim=3)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, :, 0:int(c_th.shape[3] / 2), 0:int(c_th.shape[4] / 2)]\n",
        "            c_th_i = c_th[:, :, :, int(c_th.shape[3] / 2):, 0:int(c_th.shape[4] / 2)]\n",
        "    else:\n",
        "        raise Exception('the dimension is not defined for Th_comp_matmul.')\n",
        "\n",
        "    return c_th_r, c_th_i\n",
        "\n",
        "def Th_inv(Ar, Ai):  # Complex inverse pytorch function ########\n",
        "    Ar_inv = torch.inverse(Ar + torch.matmul(torch.matmul(Ai, torch.inverse(Ar)), Ai))\n",
        "    Ai_inv = - torch.matmul(torch.matmul(torch.inverse(Ar), Ai), Ar_inv)\n",
        "    return Ar_inv, Ai_inv\n",
        "\n",
        "def Th_pinv(Ar, Ai):  # Complex inverse pytorch function ########\n",
        "    if Ar.ndim == 2:\n",
        "        if Ar.shape[0] < Ar.shape[1]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar, Ai, Ar.T, -Ai.T)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar.T, -Ai.T, Ar_inv, Ai_inv)\n",
        "        elif Ar.shape[0] > Ar.shape[1]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar.T, -Ai.T, Ar, Ai)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar_inv, Ai_inv, Ar.T, -Ai.T)\n",
        "        elif Ar.shape[0] == Ar.shape[1]:\n",
        "            return Th_inv(Ar, Ai)\n",
        "    elif Ar.ndim == 3:\n",
        "        if Ar.shape[1] < Ar.shape[2]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar, Ai, Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1))\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1), Ar_inv, Ai_inv)\n",
        "        elif Ar.shape[1] > Ar.shape[2]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1), Ar, Ai)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar_inv, Ai_inv, Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1))\n",
        "        elif Ar.shape[1] == Ar.shape[2]:\n",
        "            return Th_inv(Ar, Ai)\n",
        "    elif Ar.ndim == 4:\n",
        "        if Ar.shape[2] < Ar.shape[3]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar, Ai, Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2))\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2), Ar_inv, Ai_inv)\n",
        "        elif Ar.shape[2] > Ar.shape[3]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2), Ar, Ai)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar_inv, Ai_inv, Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2))\n",
        "        elif Ar.shape[2] == Ar.shape[3]:\n",
        "            return Th_inv(Ar, Ai)\n",
        "    else:\n",
        "        raise Exception('5-D is not defined for Th_pinv.')\n"
      ],
      "metadata": {
        "id": "Le9Kg7vW-KsE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune-client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVTXgreG-bHH",
        "outputId": "04332af0-9ae3-4c3f-c900-4d4dac53ea9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune-client\n",
            "  Downloading neptune_client-1.10.2-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.1/502.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=2.0.8 (from neptune-client)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (9.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from neptune-client) (2.3.0)\n",
            "Collecting boto3>=1.28.0 (from neptune-client)\n",
            "  Downloading boto3-1.34.87-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado<12.0.0,>=11.0.0 (from neptune-client)\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (0.18.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune-client) (24.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune-client) (2.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune-client) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.16.0)\n",
            "Collecting swagger-spec-validator>=2.7.4 (from neptune-client)\n",
            "  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (4.11.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (2.0.7)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.7.0)\n",
            "Collecting botocore<1.35.0,>=1.34.87 (from boto3>=1.28.0->neptune-client)\n",
            "  Downloading botocore-1.34.87-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.0->neptune-client)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.0->neptune-client)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune-client)\n",
            "  Downloading bravado-core-6.1.1.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.0.1)\n",
            "Collecting simplejson (from bravado<12.0.0,>=11.0.0->neptune-client)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting monotonic (from bravado<12.0.0,>=11.0.0->neptune-client)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=2.0.8->neptune-client)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune-client) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune-client) (2024.2.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.19.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client) (1.25.2)\n",
            "Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.0)\n",
            "Collecting fqdn (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting rfc3339-validator (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3986-validator>0.1.0 (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting uri-template (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.13)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Building wheels for collected packages: bravado-core\n",
            "  Building wheel for bravado-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bravado-core: filename=bravado_core-6.1.1-py2.py3-none-any.whl size=67672 sha256=086cb4a5766937f9b18e9438d8c8831edecb27849c451a7a8f930198afcb1e44\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/35/4a/44ec4c358db21a5d63ed4e40f0f0012a438106f220bce4ccba\n",
            "Successfully built bravado-core\n",
            "Installing collected packages: monotonic, uri-template, types-python-dateutil, smmap, simplejson, rfc3986-validator, rfc3339-validator, jsonref, jsonpointer, jmespath, fqdn, gitdb, botocore, arrow, s3transfer, isoduration, GitPython, swagger-spec-validator, boto3, bravado-core, bravado, neptune-client\n",
            "Successfully installed GitPython-3.1.43 arrow-1.3.0 boto3-1.34.87 botocore-1.34.87 bravado-11.0.3 bravado-core-6.1.1 fqdn-1.5.1 gitdb-4.0.11 isoduration-20.11.0 jmespath-1.0.1 jsonpointer-2.4 jsonref-1.1.0 monotonic-1.6 neptune-client-1.10.2 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 s3transfer-0.10.1 simplejson-3.19.2 smmap-5.0.1 swagger-spec-validator-3.0.3 types-python-dateutil-2.9.0.20240316 uri-template-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "from termcolor import colored\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from numpy import genfromtxt\n",
        "import numpy as np\n",
        "# from utils_math import Th_comp_matmul, Th_inv, Th_pinv\n",
        "import neptune\n",
        "import re\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Database ####################################################################################################################\n",
        "class Data_Reader(data.Dataset):\n",
        "    def __init__(self, filename, Us, Mr, Nrf, K, Noise_pwr):\n",
        "\n",
        "        print(colored('You select core dataset', 'cyan'))\n",
        "        print(colored(filename, 'yellow'), 'is loading ... ')\n",
        "        np_data = np.load(filename)\n",
        "\n",
        "        self.channelR = np_data['channel'].real.astype(float)\n",
        "        self.channelI = np_data['channel'].imag.astype(float)\n",
        "\n",
        "        self.RSSI_N = np_data['RSSI_N'].real.astype(float)\n",
        "\n",
        "        self.UR = np_data['U'].real.astype(float)\n",
        "        self.UI = np_data['U'].imag.astype(float)\n",
        "\n",
        "        self.AR = np_data['A'].real.astype(float)\n",
        "        self.AI = np_data['A'].imag.astype(float)\n",
        "\n",
        "        self.WR = np_data['W'].real.astype(float)\n",
        "        self.WI = np_data['W'].imag.astype(float)\n",
        "\n",
        "        self.Noise_pwr = Noise_pwr\n",
        "\n",
        "        self.n_samples = self.channelR.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def uniq_clas(self):\n",
        "        codes = np.load('Codebook_ij.npz')['codebook']\n",
        "        NO_Class = len(codes)\n",
        "        print(colored(\"The number of Unique AP in I1: \", \"green\"), NO_Class)\n",
        "        return NO_Class\n",
        "\n",
        "    def rate_calculator_3d_np(self, FDP, channel):  #      FDP = (i, Nt, Nu, 1)         H = (i, Nu, 1, Nt)\n",
        "        W = np.einsum('nij,njk->nik', np.conj(channel), FDP)\n",
        "        diag_W = np.diagonal(np.abs(W) ** 2, axis1=1, axis2=2)\n",
        "        SINR = diag_W / (np.sum(np.abs(W) ** 2, 2) - diag_W + self.Noise_pwr)\n",
        "        userRates = np.log2(1 + SINR)\n",
        "        sumRate = userRates.sum(1)\n",
        "        return sumRate\n",
        "\n",
        "    def optimum_HBF(self):\n",
        "        A = self.AR + 1j*self.AI\n",
        "        W = self.WR + 1j*self.WI\n",
        "        channel = self.channelR + 1j*self.channelI\n",
        "        FDP_AW = np.einsum('nij,njk->nik', A, W)\n",
        "        FDP_AW = FDP_AW / np.linalg.norm(FDP_AW, axis=(1,2), keepdims=True)\n",
        "        sr_HBF = Data_Reader.rate_calculator_3d_np(self, FDP_AW, channel)\n",
        "        return sr_HBF.mean()\n",
        "\n",
        "    def optimum_FDP(self):\n",
        "        U = self.UR + 1j*self.UI\n",
        "        channel = self.channelR + 1j*self.channelI\n",
        "        U = U / np.linalg.norm(U, axis=(1,2), keepdims=True)\n",
        "        sr_FDP = Data_Reader.rate_calculator_3d_np(self, U, channel)\n",
        "        return sr_FDP.mean()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.Tensor(self.channelR[index]), torch.Tensor(self.channelI[index]), torch.Tensor(self.RSSI_N[index])\n",
        "\n",
        "# readme reader for HBF initial parameters ####################################################################################\n",
        "def md_reader(DB_name):\n",
        "    md = genfromtxt('DATASET.md', delimiter='\\n', dtype='str')\n",
        "    Us = int(re.findall(r'\\d+', md[1])[0])\n",
        "    Mr = int(re.findall(r'\\d+', md[2])[0])\n",
        "    Nrf = int(re.findall(r'\\d+', md[3])[0])\n",
        "    Ass_n = int(re.findall(r'\\d+', md[4])[0])\n",
        "    Noise_pwr = float(''.join(('1e-', str(int(int(re.findall(r'\\d+', md[6])[0]) / 10)))))\n",
        "    return Us, Mr, Nrf, Ass_n, Noise_pwr\n",
        "\n",
        "class Initialization_Model_Params(object):\n",
        "    def __init__(self,\n",
        "                 DB_name,\n",
        "                 Us,\n",
        "                 Mr,\n",
        "                 Nrf,\n",
        "                 K,\n",
        "                 K_limited,\n",
        "                 Noise_pwr,\n",
        "                 device,\n",
        "                 device_ids\n",
        "                 ):\n",
        "        self.DB_name = DB_name\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.K = K\n",
        "        self.K_limited = K_limited\n",
        "        self.Noise_pwr = Noise_pwr\n",
        "        self.device = device\n",
        "        self.dev_id = device_ids\n",
        "\n",
        "    def Data_Load(self):\n",
        "        DataBase = Data_Reader(''.join(('DataBase_', self.DB_name, '.npz')),\n",
        "                               self.Us, self.Mr, self.Nrf, self.K, self.Noise_pwr)\n",
        "        uniq_dis_label = DataBase.uniq_clas()\n",
        "        sr_HBF, sr_FDP = DataBase.optimum_HBF(), DataBase.optimum_FDP()\n",
        "        return DataBase, uniq_dis_label, sr_HBF, sr_FDP\n",
        "\n",
        "    def Code_Read(self):\n",
        "        codes = np.load('Codebook_ij.npz')['codebook']\n",
        "        # codes = genfromtxt('Codebook_ij.csv', delimiter=',', dtype='complex', skip_header=0)\n",
        "        label = np.arange(len(codes))\n",
        "        self.n_output_clas = len(codes)\n",
        "        print(colored(\"The length of the codebook: \", \"green\"), len(codes))\n",
        "        Codes_idx = np.concatenate((label[:, np.newaxis], codes), axis=1)\n",
        "        codeword_C = {}\n",
        "        index_C = []\n",
        "        for i in range(len(codes)):\n",
        "            index_C = Codes_idx[i, 0].real.astype(int)\n",
        "            icode_C = Codes_idx[i, 1:]\n",
        "            codeword_C[index_C] = icode_C\n",
        "\n",
        "        # torch tensor of codes\n",
        "        codesr = torch.from_numpy(codes.real).type(torch.float)\n",
        "        codesi = torch.from_numpy(codes.imag).type(torch.float)\n",
        "        return codeword_C, len(codes), codesr, codesi\n",
        "\n",
        "class Loss_FDP_Rate_Based(torch.nn.Module):\n",
        "    def __init__(self, Us, Mr, Nrf, Noise_pwr):\n",
        "        super(Loss_FDP_Rate_Based, self).__init__()\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.noise_power = Noise_pwr\n",
        "\n",
        "    def rate_calculator(self, u_re, u_im, channelr, channeli):\n",
        "        Wr, Wi = Th_comp_matmul(channelr, -channeli, u_re, u_im)\n",
        "        W = Wr**2 + Wi**2\n",
        "        diag_W = torch.diagonal(W, dim1=1, dim2=2)\n",
        "\n",
        "        SINR = diag_W / (torch.sum(W, 2) - diag_W + self.noise_power)\n",
        "        userRates = torch.log2(1 + SINR)\n",
        "        sumRate = userRates.sum(1)\n",
        "\n",
        "        return sumRate\n",
        "\n",
        "    def forward(self, outr, outi, channelr, channeli):\n",
        "        outr = outr.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "        outi = outi.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(outr.flatten(1) ** 2 + outi.flatten(1) ** 2, dim=1))\n",
        "        outr = (outr.flatten(1) / temp_pre.unsqueeze(1)).view(outr.shape)\n",
        "        outi = (outi.flatten(1) / temp_pre.unsqueeze(1)).view(outi.shape)\n",
        "\n",
        "        sum_rate = Loss_FDP_Rate_Based.rate_calculator(self, outr, outi, channelr, channeli)\n",
        "        return -sum_rate.mean()\n",
        "\n",
        "    def evaluate_rate(self, outr, outi, channelr, channeli):\n",
        "        outr = outr.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "        outi = outi.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(outr.flatten(1) ** 2 + outi.flatten(1) ** 2, dim=1))\n",
        "        outr = (outr.flatten(1) / temp_pre.unsqueeze(1)).view(outr.shape)\n",
        "        outi = (outi.flatten(1) / temp_pre.unsqueeze(1)).view(outi.shape)\n",
        "\n",
        "        sum_rate = Loss_FDP_Rate_Based.rate_calculator(self, outr, outi, channelr, channeli)\n",
        "        return sum_rate.mean()\n",
        "\n",
        "class Loss_HBF_Rate_Based_4D(torch.nn.Module):\n",
        "    def __init__(self, Us, Mr, Nrf, Noise_pwr):\n",
        "        super(Loss_HBF_Rate_Based_4D, self).__init__()\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.noise_power = Noise_pwr\n",
        "\n",
        "    def rate_calculator_4d(self, u_re, u_im, channelr, channeli):\n",
        "        Wr, Wi = Th_comp_matmul(channelr, -channeli, u_re, u_im)\n",
        "        W = Wr**2 + Wi**2\n",
        "        diag_W = torch.diagonal(W, dim1=2, dim2=3)\n",
        "        SINR = diag_W / (torch.sum(W, 3) - diag_W + self.noise_power)\n",
        "        userRates = torch.log2(1 + SINR)\n",
        "        sumRate = userRates.sum(2)\n",
        "        return sumRate\n",
        "\n",
        "    def forward(self, Wr, Wi, channelr, channeli, Ar, Ai):\n",
        "        HBF_prer, HBF_prei = Th_comp_matmul(Ar.view(-1, len(channelr), self.Nrf, self.Mr).permute(0, 1, 3, 2),\n",
        "                                            Ai.view(-1, len(channelr), self.Nrf, self.Mr).permute(0, 1, 3, 2), Wr, Wi)\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(HBF_prer.flatten(2) ** 2 + HBF_prei.flatten(2) ** 2, dim=2))\n",
        "        HBF_prer = (HBF_prer.flatten(2) / temp_pre.unsqueeze(2)).view(HBF_prer.shape)\n",
        "        HBF_prei = (HBF_prei.flatten(2) / temp_pre.unsqueeze(2)).view(HBF_prei.shape)\n",
        "\n",
        "        sum_rate = Loss_HBF_Rate_Based_4D.rate_calculator_4d(self, HBF_prer, HBF_prei, channelr, channeli)\n",
        "        return sum_rate.T\n",
        "\n",
        "    def evaluate_rate(self, Wr, Wi, channelr, channeli, Ar, Ai):\n",
        "        HBF_prer, HBF_prei = Th_comp_matmul(Ar.view(-1, self.Nrf, self.Mr).permute(0, 2, 1),\n",
        "            Ai.view(-1, self.Nrf, self.Mr).permute(0, 2, 1), Wr.permute(0, 2, 1), Wi.permute(0, 2, 1))\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(HBF_prer.flatten(1) ** 2 + HBF_prei.flatten(1) ** 2, dim=1))\n",
        "        HBF_prer = (HBF_prer.flatten(1) / temp_pre.unsqueeze(1)).view(HBF_prer.shape)\n",
        "        HBF_prei = (HBF_prei.flatten(1) / temp_pre.unsqueeze(1)).view(HBF_prei.shape)\n",
        "\n",
        "        sum_rate = Loss_FDP_Rate_Based.rate_calculator(self, HBF_prer, HBF_prei, channelr, channeli)\n",
        "\n",
        "        return sum_rate.mean()\n",
        "\n",
        "def FLP_loss(x, y):\n",
        "    log_prob = - 1.0 * F.softmax(x, 1)\n",
        "    temp = log_prob * y\n",
        "    cel = temp.sum(dim=1)\n",
        "    cel = cel.mean()\n",
        "    return cel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpZTnoFc-SXc",
        "outputId": "d67875af-5511-41e8-c28e-747cc0695fa4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[neptune] [warning] NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_VISIBLE_DEVICES=1"
      ],
      "metadata": {
        "id": "zQgnqix8njLK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDUuU-a09oMc",
        "outputId": "57d8d3fb-87ed-4d31-b1a3-f74573b3188c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Cuda available?  True\n",
            "Which devide? cuda:0\n",
            "You select core dataset\n",
            "DataBase_dataSet64x8x4_130dB_0129201820.npz is loading ... \n",
            "The number of Unique AP in I1:  5\n",
            "The length of the codebook:  5\n",
            "The size of training set is  8500\n",
            "The size of Test set is  1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter:==>  1 Loss_Class:-0.343 Rate_opt_HBF:7.87 Rate_pre_HBF:0.73 Ratio_HBF:9.32%\n",
            "Iter:==>  2 Loss_Class:-3.636 Rate_opt_HBF:7.87 Rate_pre_HBF:3.43 Ratio_HBF:43.61%\n",
            "Iter:==>  3 Loss_Class:-4.013 Rate_opt_HBF:7.87 Rate_pre_HBF:4.20 Ratio_HBF:53.30%\n",
            "Iter:==>  4 Loss_Class:-4.135 Rate_opt_HBF:7.87 Rate_pre_HBF:4.46 Ratio_HBF:56.70%\n",
            "Iter:==>  5 Loss_Class:-4.226 Rate_opt_HBF:7.87 Rate_pre_HBF:4.57 Ratio_HBF:58.09%\n",
            "Iter:==>  6 Loss_Class:-4.193 Rate_opt_HBF:7.87 Rate_pre_HBF:4.63 Ratio_HBF:58.83%\n",
            "Iter:==>  7 Loss_Class:-4.298 Rate_opt_HBF:7.87 Rate_pre_HBF:4.68 Ratio_HBF:59.44%\n",
            "Iter:==>  8 Loss_Class:-4.362 Rate_opt_HBF:7.87 Rate_pre_HBF:4.69 Ratio_HBF:59.54%\n",
            "Iter:==>  9 Loss_Class:-4.470 Rate_opt_HBF:7.87 Rate_pre_HBF:4.71 Ratio_HBF:59.87%\n",
            "Iter:==> 10 Loss_Class:-4.445 Rate_opt_HBF:7.87 Rate_pre_HBF:4.73 Ratio_HBF:60.09%\n",
            "Iter:==> 11 Loss_Class:-4.420 Rate_opt_HBF:7.87 Rate_pre_HBF:4.74 Ratio_HBF:60.23%\n",
            "Iter:==> 12 Loss_Class:-4.577 Rate_opt_HBF:7.87 Rate_pre_HBF:4.76 Ratio_HBF:60.41%\n",
            "Iter:==> 13 Loss_Class:-4.525 Rate_opt_HBF:7.87 Rate_pre_HBF:4.76 Ratio_HBF:60.43%\n",
            "Iter:==> 14 Loss_Class:-4.462 Rate_opt_HBF:7.87 Rate_pre_HBF:4.78 Ratio_HBF:60.72%\n",
            "Iter:==> 15 Loss_Class:-4.511 Rate_opt_HBF:7.87 Rate_pre_HBF:4.79 Ratio_HBF:60.87%\n",
            "Iter:==> 16 Loss_Class:-4.702 Rate_opt_HBF:7.87 Rate_pre_HBF:4.84 Ratio_HBF:61.47%\n",
            "Iter:==> 17 Loss_Class:-4.681 Rate_opt_HBF:7.87 Rate_pre_HBF:4.85 Ratio_HBF:61.57%\n",
            "Iter:==> 18 Loss_Class:-4.732 Rate_opt_HBF:7.87 Rate_pre_HBF:4.88 Ratio_HBF:62.01%\n",
            "Iter:==> 19 Loss_Class:-4.711 Rate_opt_HBF:7.87 Rate_pre_HBF:4.87 Ratio_HBF:61.81%\n",
            "Iter:==> 20 Loss_Class:-4.588 Rate_opt_HBF:7.87 Rate_pre_HBF:4.78 Ratio_HBF:60.71%\n",
            "Iter:==> 21 Loss_Class:-4.738 Rate_opt_HBF:7.87 Rate_pre_HBF:4.86 Ratio_HBF:61.73%\n",
            "Iter:==> 22 Loss_Class:-4.777 Rate_opt_HBF:7.87 Rate_pre_HBF:4.89 Ratio_HBF:62.06%\n",
            "Iter:==> 23 Loss_Class:-4.798 Rate_opt_HBF:7.87 Rate_pre_HBF:4.91 Ratio_HBF:62.35%\n",
            "Iter:==> 24 Loss_Class:-4.696 Rate_opt_HBF:7.87 Rate_pre_HBF:4.90 Ratio_HBF:62.30%\n",
            "Iter:==> 25 Loss_Class:-4.758 Rate_opt_HBF:7.87 Rate_pre_HBF:4.87 Ratio_HBF:61.86%\n",
            "Iter:==> 26 Loss_Class:-4.672 Rate_opt_HBF:7.87 Rate_pre_HBF:4.86 Ratio_HBF:61.76%\n",
            "Iter:==> 27 Loss_Class:-4.828 Rate_opt_HBF:7.87 Rate_pre_HBF:4.91 Ratio_HBF:62.36%\n",
            "Iter:==> 28 Loss_Class:-4.818 Rate_opt_HBF:7.87 Rate_pre_HBF:4.87 Ratio_HBF:61.82%\n",
            "Iter:==> 29 Loss_Class:-4.774 Rate_opt_HBF:7.87 Rate_pre_HBF:4.91 Ratio_HBF:62.34%\n",
            "Iter:==> 30 Loss_Class:-4.827 Rate_opt_HBF:7.87 Rate_pre_HBF:4.90 Ratio_HBF:62.26%\n",
            "Iter:==> 31 Loss_Class:-4.822 Rate_opt_HBF:7.87 Rate_pre_HBF:4.89 Ratio_HBF:62.15%\n",
            "Iter:==> 32 Loss_Class:-4.819 Rate_opt_HBF:7.87 Rate_pre_HBF:4.91 Ratio_HBF:62.37%\n",
            "Iter:==> 33 Loss_Class:-4.840 Rate_opt_HBF:7.87 Rate_pre_HBF:4.92 Ratio_HBF:62.51%\n",
            "Iter:==> 34 Loss_Class:-4.884 Rate_opt_HBF:7.87 Rate_pre_HBF:4.92 Ratio_HBF:62.43%\n",
            "Iter:==> 35 Loss_Class:-4.875 Rate_opt_HBF:7.87 Rate_pre_HBF:4.90 Ratio_HBF:62.21%\n",
            "Iter:==> 36 Loss_Class:-4.858 Rate_opt_HBF:7.87 Rate_pre_HBF:4.88 Ratio_HBF:62.03%\n",
            "Iter:==> 37 Loss_Class:-4.824 Rate_opt_HBF:7.87 Rate_pre_HBF:4.90 Ratio_HBF:62.28%\n",
            "Iter:==> 38 Loss_Class:-4.841 Rate_opt_HBF:7.87 Rate_pre_HBF:4.92 Ratio_HBF:62.43%\n",
            "Iter:==> 39 Loss_Class:-4.858 Rate_opt_HBF:7.87 Rate_pre_HBF:4.94 Ratio_HBF:62.69%\n",
            "Iter:==> 40 Loss_Class:-4.814 Rate_opt_HBF:7.87 Rate_pre_HBF:4.93 Ratio_HBF:62.57%\n",
            "Iter:==> 41 Loss_Class:-4.862 Rate_opt_HBF:7.87 Rate_pre_HBF:4.94 Ratio_HBF:62.72%\n",
            "Iter:==> 42 Loss_Class:-4.859 Rate_opt_HBF:7.87 Rate_pre_HBF:4.91 Ratio_HBF:62.32%\n",
            "Iter:==> 43 Loss_Class:-4.896 Rate_opt_HBF:7.87 Rate_pre_HBF:4.94 Ratio_HBF:62.73%\n",
            "Iter:==> 44 Loss_Class:-4.881 Rate_opt_HBF:7.87 Rate_pre_HBF:4.94 Ratio_HBF:62.75%\n",
            "Iter:==> 45 Loss_Class:-4.904 Rate_opt_HBF:7.87 Rate_pre_HBF:4.93 Ratio_HBF:62.62%\n",
            "Iter:==> 46 Loss_Class:-4.912 Rate_opt_HBF:7.87 Rate_pre_HBF:4.93 Ratio_HBF:62.60%\n",
            "Iter:==> 47 Loss_Class:-4.864 Rate_opt_HBF:7.87 Rate_pre_HBF:4.92 Ratio_HBF:62.47%\n",
            "Iter:==> 48 Loss_Class:-4.882 Rate_opt_HBF:7.87 Rate_pre_HBF:4.94 Ratio_HBF:62.79%\n",
            "Iter:==> 49 Loss_Class:-4.904 Rate_opt_HBF:7.87 Rate_pre_HBF:4.94 Ratio_HBF:62.73%\n",
            "Iter:==> 50 Loss_Class:-4.920 Rate_opt_HBF:7.87 Rate_pre_HBF:4.94 Ratio_HBF:62.79%\n",
            "Iter:==> 51 Loss_Class:-4.899 Rate_opt_HBF:7.87 Rate_pre_HBF:4.94 Ratio_HBF:62.79%\n",
            "Iter:==> 52 Loss_Class:-4.906 Rate_opt_HBF:7.87 Rate_pre_HBF:4.95 Ratio_HBF:62.81%\n",
            "Iter:==> 53 Loss_Class:-4.907 Rate_opt_HBF:7.87 Rate_pre_HBF:4.94 Ratio_HBF:62.77%\n",
            "Iter:==> 54 Loss_Class:-4.922 Rate_opt_HBF:7.87 Rate_pre_HBF:4.95 Ratio_HBF:62.87%\n",
            "Iter:==> 55 Loss_Class:-4.915 Rate_opt_HBF:7.87 Rate_pre_HBF:4.95 Ratio_HBF:62.91%\n",
            "Iter:==> 56 Loss_Class:-4.902 Rate_opt_HBF:7.87 Rate_pre_HBF:4.95 Ratio_HBF:62.82%\n",
            "Iter:==> 57 Loss_Class:-4.926 Rate_opt_HBF:7.87 Rate_pre_HBF:4.95 Ratio_HBF:62.82%\n",
            "Iter:==> 58 Loss_Class:-4.919 Rate_opt_HBF:7.87 Rate_pre_HBF:4.93 Ratio_HBF:62.64%\n",
            "Iter:==> 59 Loss_Class:-4.893 Rate_opt_HBF:7.87 Rate_pre_HBF:4.93 Ratio_HBF:62.66%\n",
            "Iter:==> 60 Loss_Class:-4.887 Rate_opt_HBF:7.87 Rate_pre_HBF:4.95 Ratio_HBF:62.81%\n",
            "Iter:==> 61 Loss_Class:-4.930 Rate_opt_HBF:7.87 Rate_pre_HBF:4.95 Ratio_HBF:62.88%\n",
            "Iter:==> 62 Loss_Class:-4.934 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.15%\n",
            "Iter:==> 63 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.14%\n",
            "Iter:==> 64 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.11%\n",
            "Iter:==> 65 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.10%\n",
            "Iter:==> 66 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==> 67 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.18%\n",
            "Iter:==> 68 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.17%\n",
            "Iter:==> 69 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==> 70 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==> 71 Loss_Class:-4.920 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==> 72 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.13%\n",
            "Iter:==> 73 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==> 74 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.17%\n",
            "Iter:==> 75 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.16%\n",
            "Iter:==> 76 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==> 77 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==> 78 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==> 79 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==> 80 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==> 81 Loss_Class:-4.935 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==> 82 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==> 83 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==> 84 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==> 85 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==> 86 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==> 87 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==> 88 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==> 89 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==> 90 Loss_Class:-4.973 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==> 91 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==> 92 Loss_Class:-4.920 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.18%\n",
            "Iter:==> 93 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==> 94 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==> 95 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==> 96 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==> 97 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==> 98 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==> 99 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>100 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>101 Loss_Class:-4.920 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>102 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>103 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>104 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>105 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>106 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>107 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>108 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>109 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>110 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>111 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>112 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>113 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>114 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>115 Loss_Class:-4.974 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>116 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>117 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>118 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>119 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>120 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>121 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>122 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>123 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>124 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>125 Loss_Class:-4.971 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>126 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>127 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>128 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>129 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>130 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>131 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>132 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>133 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>134 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>135 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>136 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>137 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>138 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>139 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>140 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>141 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>142 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>143 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>144 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>145 Loss_Class:-4.971 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>146 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>147 Loss_Class:-4.936 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>148 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>149 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>150 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>151 Loss_Class:-4.916 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>152 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>153 Loss_Class:-4.936 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>154 Loss_Class:-4.933 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>155 Loss_Class:-4.921 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>156 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>157 Loss_Class:-4.976 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>158 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>159 Loss_Class:-4.917 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>160 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>161 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>162 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>163 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>164 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>165 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>166 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>167 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>168 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>169 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>170 Loss_Class:-4.936 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>171 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>172 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>173 Loss_Class:-4.922 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>174 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>175 Loss_Class:-4.918 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>176 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>177 Loss_Class:-4.930 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>178 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>179 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>180 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>181 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>182 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>183 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>184 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>185 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>186 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>187 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>188 Loss_Class:-4.916 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>189 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>190 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>191 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>192 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>193 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>194 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>195 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>196 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>197 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>198 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>199 Loss_Class:-4.909 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>200 Loss_Class:-4.936 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>201 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>202 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>203 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>204 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>205 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>206 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>207 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>208 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>209 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>210 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>211 Loss_Class:-4.934 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>212 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>213 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>214 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>215 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>216 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>217 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>218 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>219 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>220 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>221 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>222 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>223 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>224 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>225 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>226 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>227 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>228 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>229 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>230 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>231 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>232 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>233 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>234 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>235 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>236 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>237 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>238 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.19%\n",
            "Iter:==>239 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>240 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>241 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>242 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>243 Loss_Class:-4.897 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>244 Loss_Class:-4.936 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>245 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>246 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>247 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>248 Loss_Class:-4.926 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>249 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>250 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>251 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>252 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>253 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>254 Loss_Class:-4.929 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>255 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>256 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>257 Loss_Class:-4.932 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>258 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>259 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>260 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>261 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>262 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>263 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>264 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>265 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>266 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>267 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>268 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>269 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>270 Loss_Class:-4.929 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>271 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>272 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>273 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>274 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.19%\n",
            "Iter:==>275 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>276 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>277 Loss_Class:-4.935 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>278 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>279 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>280 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>281 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>282 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>283 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>284 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>285 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>286 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>287 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>288 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>289 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>290 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>291 Loss_Class:-4.973 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>292 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>293 Loss_Class:-4.972 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>294 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>295 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>296 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>297 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>298 Loss_Class:-4.974 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>299 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>300 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>301 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>302 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>303 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>304 Loss_Class:-4.932 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>305 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>306 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>307 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>308 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>309 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>310 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>311 Loss_Class:-4.929 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>312 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>313 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>314 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>315 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>316 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>317 Loss_Class:-4.913 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>318 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>319 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>320 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>321 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>322 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>323 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>324 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>325 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>326 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>327 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>328 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>329 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>330 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>331 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>332 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>333 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>334 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>335 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>336 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>337 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>338 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>339 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>340 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>341 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>342 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>343 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>344 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>345 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>346 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>347 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>348 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>349 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>350 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>351 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>352 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>353 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>354 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>355 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>356 Loss_Class:-4.929 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>357 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>358 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>359 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>360 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.28%\n",
            "Iter:==>361 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>362 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>363 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>364 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>365 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>366 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>367 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>368 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>369 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>370 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>371 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>372 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>373 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>374 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>375 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>376 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>377 Loss_Class:-4.972 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>378 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>379 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>380 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>381 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>382 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>383 Loss_Class:-4.898 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.19%\n",
            "Iter:==>384 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>385 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>386 Loss_Class:-4.971 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>387 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>388 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>389 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>390 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>391 Loss_Class:-4.937 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>392 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>393 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>394 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>395 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>396 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>397 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>398 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>399 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>400 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>401 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>402 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>403 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>404 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>405 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>406 Loss_Class:-4.923 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>407 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>408 Loss_Class:-4.936 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>409 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>410 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>411 Loss_Class:-4.981 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>412 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>413 Loss_Class:-4.927 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>414 Loss_Class:-4.927 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>415 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>416 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.19%\n",
            "Iter:==>417 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>418 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>419 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>420 Loss_Class:-4.935 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>421 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>422 Loss_Class:-4.975 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>423 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>424 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>425 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>426 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>427 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>428 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>429 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>430 Loss_Class:-4.972 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>431 Loss_Class:-4.933 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>432 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>433 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>434 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>435 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>436 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>437 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>438 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>439 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>440 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>441 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>442 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>443 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>444 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>445 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>446 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>447 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>448 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>449 Loss_Class:-4.936 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>450 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>451 Loss_Class:-4.978 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>452 Loss_Class:-4.911 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>453 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>454 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>455 Loss_Class:-4.908 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>456 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>457 Loss_Class:-4.923 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>458 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.19%\n",
            "Iter:==>459 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>460 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>461 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>462 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>463 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>464 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>465 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>466 Loss_Class:-4.931 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>467 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>468 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>469 Loss_Class:-4.937 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>470 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>471 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>472 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>473 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>474 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>475 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>476 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>477 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>478 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>479 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>480 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>481 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>482 Loss_Class:-4.932 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>483 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>484 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>485 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>486 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>487 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>488 Loss_Class:-4.932 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>489 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>490 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>491 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>492 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>493 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.18%\n",
            "Iter:==>494 Loss_Class:-4.934 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>495 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>496 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>497 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>498 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>499 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>500 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>501 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>502 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>503 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>504 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>505 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>506 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>507 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>508 Loss_Class:-4.972 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>509 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>510 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>511 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>512 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>513 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>514 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>515 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>516 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>517 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>518 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>519 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>520 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>521 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>522 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>523 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>524 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>525 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>526 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>527 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>528 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>529 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>530 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>531 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>532 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>533 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>534 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>535 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>536 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>537 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>538 Loss_Class:-4.934 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>539 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>540 Loss_Class:-4.935 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>541 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>542 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>543 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>544 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>545 Loss_Class:-4.936 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>546 Loss_Class:-4.895 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.19%\n",
            "Iter:==>547 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>548 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>549 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>550 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>551 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>552 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>553 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>554 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>555 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>556 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>557 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>558 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>559 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>560 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>561 Loss_Class:-4.907 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>562 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>563 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>564 Loss_Class:-4.922 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>565 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>566 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>567 Loss_Class:-4.972 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>568 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>569 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>570 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>571 Loss_Class:-4.973 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>572 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>573 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>574 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>575 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>576 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>577 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>578 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>579 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>580 Loss_Class:-4.927 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>581 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>582 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>583 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>584 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>585 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>586 Loss_Class:-4.925 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>587 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>588 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>589 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>590 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>591 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>592 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>593 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>594 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>595 Loss_Class:-4.932 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>596 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>597 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>598 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>599 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>600 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>601 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>602 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>603 Loss_Class:-4.925 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>604 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>605 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>606 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>607 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>608 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>609 Loss_Class:-4.974 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>610 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>611 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>612 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>613 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>614 Loss_Class:-4.905 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>615 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>616 Loss_Class:-4.923 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>617 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>618 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>619 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>620 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>621 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>622 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>623 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>624 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>625 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>626 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>627 Loss_Class:-4.972 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>628 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>629 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>630 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>631 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>632 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>633 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>634 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>635 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>636 Loss_Class:-4.976 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>637 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>638 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>639 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>640 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>641 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.19%\n",
            "Iter:==>642 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>643 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>644 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>645 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>646 Loss_Class:-4.973 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>647 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>648 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>649 Loss_Class:-4.933 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>650 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>651 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>652 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>653 Loss_Class:-4.931 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>654 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>655 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>656 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>657 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>658 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>659 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>660 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>661 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>662 Loss_Class:-4.923 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>663 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>664 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>665 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>666 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>667 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>668 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>669 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>670 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>671 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>672 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>673 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>674 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>675 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>676 Loss_Class:-4.913 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>677 Loss_Class:-4.934 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>678 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>679 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>680 Loss_Class:-4.906 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>681 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>682 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>683 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>684 Loss_Class:-4.926 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>685 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>686 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>687 Loss_Class:-4.920 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>688 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>689 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>690 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>691 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>692 Loss_Class:-4.933 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>693 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>694 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>695 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>696 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>697 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>698 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>699 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>700 Loss_Class:-4.932 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>701 Loss_Class:-4.974 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>702 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.28%\n",
            "Iter:==>703 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>704 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>705 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>706 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>707 Loss_Class:-4.920 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>708 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>709 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>710 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>711 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>712 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>713 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>714 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>715 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>716 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>717 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>718 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>719 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>720 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>721 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>722 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>723 Loss_Class:-4.937 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>724 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>725 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>726 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>727 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>728 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>729 Loss_Class:-4.918 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>730 Loss_Class:-4.922 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>731 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>732 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>733 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>734 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>735 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>736 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>737 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>738 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>739 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>740 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>741 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>742 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>743 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>744 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>745 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>746 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>747 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>748 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>749 Loss_Class:-4.975 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>750 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>751 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>752 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>753 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>754 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>755 Loss_Class:-4.971 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>756 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>757 Loss_Class:-4.937 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>758 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>759 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>760 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>761 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>762 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>763 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>764 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>765 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>766 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>767 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>768 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>769 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>770 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>771 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>772 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>773 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>774 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>775 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>776 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>777 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>778 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>779 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>780 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>781 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>782 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>783 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>784 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>785 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>786 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>787 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>788 Loss_Class:-4.970 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>789 Loss_Class:-4.932 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>790 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>791 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>792 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>793 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>794 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>795 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>796 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>797 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>798 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>799 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>800 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>801 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>802 Loss_Class:-4.925 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>803 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>804 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>805 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>806 Loss_Class:-4.937 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>807 Loss_Class:-4.925 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>808 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>809 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>810 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>811 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>812 Loss_Class:-4.974 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>813 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>814 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>815 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>816 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>817 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>818 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>819 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>820 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>821 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>822 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>823 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>824 Loss_Class:-4.971 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>825 Loss_Class:-4.916 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>826 Loss_Class:-4.937 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>827 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>828 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>829 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>830 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>831 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>832 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>833 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>834 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>835 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>836 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>837 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>838 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>839 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>840 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>841 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>842 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>843 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>844 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>845 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>846 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>847 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>848 Loss_Class:-4.859 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>849 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>850 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>851 Loss_Class:-4.935 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.19%\n",
            "Iter:==>852 Loss_Class:-4.971 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>853 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>854 Loss_Class:-4.929 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>855 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>856 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>857 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>858 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>859 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>860 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.17%\n",
            "Iter:==>861 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>862 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>863 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>864 Loss_Class:-4.972 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>865 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>866 Loss_Class:-4.921 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>867 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>868 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>869 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>870 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>871 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>872 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>873 Loss_Class:-4.947 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>874 Loss_Class:-4.931 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>875 Loss_Class:-4.912 Rate_opt_HBF:7.87 Rate_pre_HBF:4.97 Ratio_HBF:63.18%\n",
            "Iter:==>876 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>877 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>878 Loss_Class:-4.923 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>879 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>880 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>881 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>882 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>883 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>884 Loss_Class:-4.935 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>885 Loss_Class:-4.975 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>886 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>887 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>888 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>889 Loss_Class:-4.936 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>890 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>891 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>892 Loss_Class:-4.977 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>893 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>894 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>895 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>896 Loss_Class:-4.932 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>897 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>898 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>899 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>900 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>901 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>902 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>903 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>904 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>905 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>906 Loss_Class:-4.944 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>907 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>908 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>909 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>910 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>911 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>912 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>913 Loss_Class:-4.969 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>914 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>915 Loss_Class:-4.942 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>916 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>917 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>918 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>919 Loss_Class:-4.934 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>920 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>921 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>922 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>923 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>924 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>925 Loss_Class:-4.976 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>926 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>927 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>928 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>929 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>930 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>931 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>932 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>933 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>934 Loss_Class:-4.973 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>935 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>936 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>937 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>938 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>939 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>940 Loss_Class:-4.927 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>941 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>942 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>943 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>944 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>945 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>946 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>947 Loss_Class:-4.939 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>948 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>949 Loss_Class:-4.973 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>950 Loss_Class:-4.961 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>951 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.27%\n",
            "Iter:==>952 Loss_Class:-4.946 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>953 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>954 Loss_Class:-4.945 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>955 Loss_Class:-4.972 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>956 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>957 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>958 Loss_Class:-4.951 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>959 Loss_Class:-4.924 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>960 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>961 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>962 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>963 Loss_Class:-4.968 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>964 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>965 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>966 Loss_Class:-4.933 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>967 Loss_Class:-4.934 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>968 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.20%\n",
            "Iter:==>969 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>970 Loss_Class:-4.941 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>971 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>972 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>973 Loss_Class:-4.966 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>974 Loss_Class:-4.950 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>975 Loss_Class:-4.925 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>976 Loss_Class:-4.949 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>977 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>978 Loss_Class:-4.965 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>979 Loss_Class:-4.954 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>980 Loss_Class:-4.964 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.22%\n",
            "Iter:==>981 Loss_Class:-4.958 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>982 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>983 Loss_Class:-4.948 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>984 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>985 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>986 Loss_Class:-4.955 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.26%\n",
            "Iter:==>987 Loss_Class:-4.956 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>988 Loss_Class:-4.943 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>989 Loss_Class:-4.960 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>990 Loss_Class:-4.957 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.21%\n",
            "Iter:==>991 Loss_Class:-4.963 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>992 Loss_Class:-4.962 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>993 Loss_Class:-4.952 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>994 Loss_Class:-4.938 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.25%\n",
            "Iter:==>995 Loss_Class:-4.953 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>996 Loss_Class:-4.959 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>997 Loss_Class:-4.907 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.23%\n",
            "Iter:==>998 Loss_Class:-4.940 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n",
            "Iter:==>999 Loss_Class:-4.967 Rate_opt_HBF:7.87 Rate_pre_HBF:4.98 Ratio_HBF:63.24%\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Source code for paper \"Unsupervised Deep Learning for Massive MIMO Hybrid Beamforming\". https://arxiv.org/abs/2007.00038.\n",
        "Python codes prepared by Hamed Hojatian, 2020.\n",
        "E-mail me for questions via: hamed.hojatian.ca@gmail.com.\n",
        "\n",
        "There is an option to select \"HBF-Net\" or \"AFP-Net\".\n",
        "\n",
        "Feel free to use this code as a starting point for your own research project.\n",
        "If you do, we kindly ask that you cite the above paper.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy import genfromtxt\n",
        "import torch as th\n",
        "# from networks_activation import Networks_activations\n",
        "# from utils import md_reader, Initialization_Model_Params, Loss_FDP_Rate_Based, Loss_HBF_Rate_Based_4D, FLP_loss\n",
        "# from utils_math import Th_pinv, Th_comp_matmul, Th_inv\n",
        "from termcolor import colored\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "###############################################################################\n",
        "# Directory file\n",
        "###############################################################################\n",
        "DB_name = 'dataSet64x8x4_130dB_0129201820'\n",
        "\n",
        "###############################################################################\n",
        "# Processor selection GPU if available (using GPU is highly recommended)\n",
        "###############################################################################\n",
        "# device = th.device(\"cuda:2\" if th.cuda.is_available() else \"cpu\")\n",
        "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
        "device_ids = [0]\n",
        "# device_id= [0]\n",
        "print(\"Is Cuda available? \", colored('True', 'green')\n",
        "    if th.cuda.is_available() else colored('False', 'red'))\n",
        "print(\"Which devide?\", colored(device, 'cyan'))\n",
        "\n",
        "###############################################################################\n",
        "# Setup Parameters\n",
        "###############################################################################\n",
        "\n",
        "# Beamforming approach  AFP_Net, HBF_NET   ####################################\n",
        "BF_approach = 'HBF_Net'\n",
        "\n",
        "###############################################################################\n",
        "# Beamfroming system model and DNN Parameters\n",
        "###############################################################################\n",
        "import os\n",
        "\n",
        "current_working_directory = os.getcwd()\n",
        "Us, Mr, Nrf, K, Noise_pwr = md_reader(DB_name)                # Number of users, antenna, K, RF chains and noise power\n",
        "K_limited = K                                                 # Number of SS as RSSI\n",
        "batch_size = 500                                              # Batch size\n",
        "epoch_size = 1000                                             # Number of training epoches\n",
        "lr = 0.001                                                    # Learning rate\n",
        "wd = 1e-6                                                     # Weight decay\n",
        "n_input = Us * K_limited                                      # Input dimensions\n",
        "n_hidden = 1024                                               # Size of FCL layers\n",
        "out_channel = 16                                              # Size of CL channels\n",
        "kernel_s = 3                                                  # Size of Kernels in CL\n",
        "padding = 1                                                   # Size of padding in CL\n",
        "p_dropout = 0.05                                              # Probability of dropout\n",
        "\n",
        "if BF_approach == 'HBF_Net':\n",
        "    n_output_reg = Us * Nrf\n",
        "elif BF_approach == 'AFP_Net':\n",
        "    n_output_reg = Us * Mr\n",
        "else:\n",
        "    raise Exception('BF_approach value is wrong !!')\n",
        "###############################################################################\n",
        "# Main Menu of configuration\n",
        "###############################################################################\n",
        "Main_Menu = Initialization_Model_Params(DB_name,\n",
        "                                        Us,\n",
        "                                        Mr,\n",
        "                                        Nrf,\n",
        "                                        K,\n",
        "                                        K_limited,\n",
        "                                        Noise_pwr,\n",
        "                                        device,\n",
        "                                        device_ids)\n",
        "\n",
        "###############################################################################\n",
        "# Reading Database\n",
        "###############################################################################\n",
        "DataBase, uniq_dis_label, sr_HBF, sr_FDP = Main_Menu.Data_Load()\n",
        "\n",
        "###############################################################################\n",
        "# Codeword dictionary\n",
        "###############################################################################\n",
        "codeword_C, n_output_clas, codesr, codesi = Main_Menu.Code_Read()\n",
        "codesr = codesr.to(device)\n",
        "codesi = codesi.to(device)\n",
        "###############################################################################\n",
        "# Training-set and test-set generation\n",
        "###############################################################################\n",
        "train_size = int(0.85 * len(DataBase))\n",
        "test_size = len(DataBase) - train_size\n",
        "train_dataset, test_dataset = th.utils.data.random_split(DataBase, [train_size, test_size])\n",
        "\n",
        "print(colored('The size of training set is ', 'yellow'), len(train_dataset))\n",
        "print(colored('The size of Test set is ', 'yellow'), len(test_dataset))\n",
        "\n",
        "###############################################################################\n",
        "# Dataloaders\n",
        "###############################################################################\n",
        "my_dataloader = th.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "my_testloader = th.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "###############################################################################\n",
        "# DNN architecture parameters\n",
        "###############################################################################\n",
        "Networks_Main_Menu = Networks_activations(Us,\n",
        "                                        Mr,\n",
        "                                        Nrf,\n",
        "                                        K,\n",
        "                                        K_limited,\n",
        "                                        Noise_pwr,\n",
        "                                        device,\n",
        "                                        device_ids,\n",
        "                                        n_input,\n",
        "                                        n_hidden,\n",
        "                                        n_output_reg,\n",
        "                                        n_output_clas,\n",
        "                                        p_dropout,\n",
        "                                        out_channel,\n",
        "                                        kernel_s,\n",
        "                                        padding)\n",
        "\n",
        "Model_m_task = Networks_Main_Menu.Network_m_Task()\n",
        "\n",
        "###############################################################################\n",
        "# DNN OPTIMIZER\n",
        "###############################################################################\n",
        "optimizer_m_task = th.optim.Adam(Model_m_task.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "###############################################################################\n",
        "# scheduler lr\n",
        "###############################################################################\n",
        "scheduler_MT = ReduceLROnPlateau(optimizer_m_task, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "###############################################################################\n",
        "# Main training loop\n",
        "###############################################################################\n",
        "if BF_approach == 'AFP_Net':\n",
        "    # initialing the loss function\n",
        "    criterium_clas_4d = Loss_HBF_Rate_Based_4D(Us, Mr, Nrf, Noise_pwr).to(device)\n",
        "    criterium_reg = Loss_FDP_Rate_Based(Us, Mr, Nrf, Noise_pwr).to(device)\n",
        "    for i in range(1, epoch_size):   # Main traning loop\n",
        "        for k, (channelR, channelI, RSSI) in enumerate(my_dataloader):  # Loading data from data loader\n",
        "\n",
        "            # Input data dimension check (CNN)\n",
        "            Inputs_MT = Networks_Main_Menu.Inp_MT(RSSI)\n",
        "\n",
        "            # Loading the CSI (real and imaginary)\n",
        "            channelR = channelR.view(-1, Us, Mr).to(device)\n",
        "            channelI = channelI.view(-1, Us, Mr).to(device)\n",
        "\n",
        "            # Set gradient to 0.\n",
        "            optimizer_m_task.zero_grad()\n",
        "\n",
        "            # Feed forward multi-tasking DNN\n",
        "            Model_m_task.train()\n",
        "            out1_reg, out2_reg, out_clas = Model_m_task(Inputs_MT)\n",
        "\n",
        "            # Computing loss for FDP in AFP-Net eq(27) in the paper\n",
        "            loss_reg = criterium_reg(out1_reg, out2_reg, channelR, channelI)\n",
        "\n",
        "            # computing the loss fucntion for HBF using eq(20)\n",
        "            xx_pr, xx_pi = Th_pinv(th.unsqueeze(codesr.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).view(-1, len(RSSI), Nrf, Mr).to(device),\n",
        "                th.unsqueeze(codesi.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).view(-1, len(RSSI), Nrf, Mr).to(device))\n",
        "            w_outr, w_outi = Th_comp_matmul(out1_reg.view(-1, Us, Mr), out2_reg.view(-1, Us, Mr), xx_pr, xx_pi)\n",
        "\n",
        "            HBF_all_4d = criterium_clas_4d(w_outr.permute(0, 1, 3, 2), w_outi.permute(0, 1, 3, 2), channelR, channelI,\n",
        "                th.unsqueeze(codesr.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).to(device), th.unsqueeze(codesi.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).to(device))\n",
        "\n",
        "            loss_clas = FLP_loss(out_clas, HBF_all_4d)\n",
        "\n",
        "            # total loss fucntion eq(29)\n",
        "            loss = loss_clas + loss_reg\n",
        "\n",
        "            # Gradient calculation.\n",
        "            loss_clas.backward(retain_graph=True)\n",
        "            loss_reg.backward(retain_graph=True)\n",
        "            loss.backward()\n",
        "\n",
        "            # Model weight modification based on the optimizer.\n",
        "            optimizer_m_task.step()\n",
        "\n",
        "            # iterate through test dataset\n",
        "            if k == 0 or i % epoch_size == 0:\n",
        "                del loss\n",
        "                # No gardient in test mode\n",
        "                with th.no_grad():\n",
        "                    R_predicted_HBF = []\n",
        "                    R_optimum_HBF = []\n",
        "                    R_optimum_FDP = []\n",
        "                    R_predicted_FDP = []\n",
        "                    Rate_Ratio_HBF = []\n",
        "                    Rate_Ratio_FDP = []\n",
        "                    for (tchannelR, tchannelI, tRSSI) in my_testloader:\n",
        "\n",
        "                        # Input data dimension check (CNN)\n",
        "                        testInputs_Reg = Networks_Main_Menu.Inp_MT(tRSSI)\n",
        "\n",
        "                        # Loading the near-optimal digital precoder, CSI (real and imaginary)\n",
        "                        T_channelR = tchannelR.reshape(-1, Us, Mr).to(device)\n",
        "                        T_channelI = tchannelI.reshape(-1, Us, Mr).to(device)\n",
        "\n",
        "                        # Forward pass test mode DNN\n",
        "                        Model_m_task.eval()\n",
        "                        pred1_reg, pred2_reg, pred_class = Model_m_task(testInputs_Reg)\n",
        "\n",
        "                        # find the maximum probability as predication of classification\n",
        "                        _, predicted = th.max(F.softmax(pred_class, 1), 1)\n",
        "\n",
        "                        # mapping in the codebook to find the corresponding analog precoder\n",
        "                        An_Predr = codesr[predicted, :].to(device)\n",
        "                        An_Predi = codesi[predicted, :].to(device)\n",
        "\n",
        "                        # finding digital precoder using eq(20)\n",
        "                        x_pr, x_pi = Th_pinv(An_Predr.view(-1, Nrf, Mr), An_Predi.view(-1, Nrf, Mr))\n",
        "                        w_prer, w_prei = Th_comp_matmul(pred1_reg.view(-1, Us, Mr), pred2_reg.view(-1, Us, Mr), x_pr, x_pi)\n",
        "\n",
        "                        # rate calculation\n",
        "                        # DNN HBF\n",
        "                        R_predicted_HBF.append(criterium_clas_4d.evaluate_rate(w_prer, w_prei, T_channelR, T_channelI, An_Predr, An_Predi))\n",
        "                        # DNN FDP\n",
        "                        R_predicted_FDP.append(criterium_reg.evaluate_rate(pred1_reg, pred2_reg, T_channelR, T_channelI))\n",
        "\n",
        "                # Average over all mini-batches\n",
        "                RATE_Predicted_HBF = sum(R_predicted_HBF) / len(R_predicted_HBF)\n",
        "                RATE_Predicted_FDP = sum(R_predicted_FDP) / len(R_predicted_FDP)\n",
        "                RATE_Ratie_HBF = 100 * RATE_Predicted_HBF / sr_HBF\n",
        "                RATE_Ratie_FDP = 100 * RATE_Predicted_FDP / sr_FDP\n",
        "\n",
        "                scheduler_MT.step(RATE_Predicted_HBF)\n",
        "\n",
        "                print('Iter:==>{:3d} Loss_FDP:{:.3f} Loss_Class:{:.3f} Rate_opt_HBF:{:.2f} Rate_opt_FDP:{:.2f} Rate_pre_HBF:{:.2f} Rate_pre_FDP:{:.2f} Ratio_HBF:{:.2f}% Ratio_FDP:{:.2f}%'.\n",
        "                    format(i, loss_reg, loss_clas, sr_HBF, sr_FDP, RATE_Predicted_HBF, RATE_Predicted_FDP, RATE_Ratie_HBF, RATE_Ratie_FDP))\n",
        "\n",
        "elif BF_approach == 'HBF_Net':\n",
        "    # initialing the loss function\n",
        "    criterium_clas_4d = Loss_HBF_Rate_Based_4D(Us, Mr, Nrf, Noise_pwr).to(device)\n",
        "    for i in range(1, epoch_size):\n",
        "        for k, (channelR, channelI, RSSI) in enumerate(my_dataloader):\n",
        "\n",
        "            # Input data dimension check (CNN)\n",
        "            Inputs_Reg = Networks_Main_Menu.Inp_MT(RSSI)\n",
        "\n",
        "            # Loading the CSI (real and imaginary)\n",
        "            channelR = channelR.view(-1, Us, Mr).to(device)\n",
        "            channelI = channelI.view(-1, Us, Mr).to(device)\n",
        "\n",
        "            # Set gradient to 0.\n",
        "            optimizer_m_task.zero_grad()\n",
        "\n",
        "            # Feed forward multi-tasking DNN\n",
        "            Model_m_task.train()\n",
        "            out1_reg, out2_reg, out_clas = Model_m_task(Inputs_Reg)\n",
        "\n",
        "            # computing the loss fucntion for HBF using eq(25)\n",
        "            w_outr, w_outi = out1_reg.view(-1, Us, Nrf), out2_reg.view(-1, Us, Nrf)\n",
        "            HBF_all_4d = criterium_clas_4d(w_outr.permute(0, 2, 1), w_outi.permute(0, 2, 1), channelR, channelI,\n",
        "                th.unsqueeze(codesr.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).to(device),\n",
        "                th.unsqueeze(codesi.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).to(device))\n",
        "            loss_clas = FLP_loss(out_clas, HBF_all_4d)\n",
        "\n",
        "            # Gradient calculation.\n",
        "            loss_clas.backward()\n",
        "\n",
        "            # Model weight modification based on the optimizer.\n",
        "            optimizer_m_task.step()\n",
        "\n",
        "            # iterate through test dataset\n",
        "            if k == 0 or i % epoch_size == 0:\n",
        "                R_predicted_HBF = []\n",
        "                R_optimum_HBF = []\n",
        "                Rate_Ratio_HBF = []\n",
        "                with th.no_grad():\n",
        "                    for (tchannelR, tchannelI, tRSSI) in my_testloader:\n",
        "\n",
        "                        # Input data dimension check (CNN)\n",
        "                        testInputs_Reg = Networks_Main_Menu.Inp_MT(tRSSI)\n",
        "\n",
        "                        # Loading the near-optimal digital precoder, CSI (real and imaginary)\n",
        "                        T_channelR = tchannelR.reshape(-1, Us, Mr).to(device)\n",
        "                        T_channelI = tchannelI.reshape(-1, Us, Mr).to(device)\n",
        "\n",
        "                        # Forward pass reg\n",
        "                        Model_m_task.eval()\n",
        "                        pred1_reg, pred2_reg, pred_class = Model_m_task(testInputs_Reg)\n",
        "\n",
        "                        # find the maximum probability as predication of classification\n",
        "                        _, predicted = th.max(F.softmax(pred_class, 1), 1)\n",
        "\n",
        "                        # mapping in the codebook to find the corresponding analog precoder\n",
        "                        An_Predr = codesr[predicted, :]\n",
        "                        An_Predi = codesi[predicted, :]\n",
        "                        w_prer, w_prei = pred1_reg.view(-1, Us, Nrf), pred2_reg.view(-1, Us, Nrf)\n",
        "\n",
        "                        # rate calculation\n",
        "                        # DNN HBF\n",
        "                        R_predicted_HBF.append(criterium_clas_4d.evaluate_rate(w_prer, w_prei, T_channelR, T_channelI, An_Predr, An_Predi))\n",
        "\n",
        "                # Average over all mini-batches\n",
        "                RATE_Predicted_HBF = sum(R_predicted_HBF) / len(R_predicted_HBF)\n",
        "                RATE_Ratie_HBF = 100 * RATE_Predicted_HBF / sr_HBF\n",
        "\n",
        "                scheduler_MT.step(RATE_Predicted_HBF)\n",
        "\n",
        "                print('Iter:==>{:3d} Loss_Class:{:.3f} Rate_opt_HBF:{:.2f} Rate_pre_HBF:{:.2f} Ratio_HBF:{:.2f}%'.\n",
        "                    format(i, loss_clas, sr_HBF, RATE_Predicted_HBF, RATE_Ratie_HBF))\n",
        "\n",
        "else:\n",
        "    raise Exception('BF_approach is wrong !!')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdWWe5EDpEs2",
        "outputId": "266b9df2-f98d-489c-dddb-8507e7686f91"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdViTaSSpKGt",
        "outputId": "c3a86fab-44d2-4ed4-dc4e-35736a4d45f9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    # Get the number of available CUDA devices\n",
        "    num_devices = torch.cuda.device_count()\n",
        "    print(\"Number of CUDA devices:\", num_devices)\n",
        "\n",
        "    # Iterate over each CUDA device and print its index\n",
        "    for i in range(num_devices):\n",
        "        device_name = torch.cuda.get_device_name(i)\n",
        "        print(f\"Device ordinal for '{device_name}': cuda:{i}\")\n",
        "else:\n",
        "    print(\"CUDA is not available on this system.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7CUatC4pZcV",
        "outputId": "15cbe462-cf64-44e7-bd3c-6623093f34b2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CUDA devices: 1\n",
            "Device ordinal for 'Tesla T4': cuda:0\n"
          ]
        }
      ]
    }
  ]
}